{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Essential Libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vibhutha/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = \"MNIST_data/\"\n",
    "mnist = input_data.read_data_sets(LOGDIR + \"data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Connected Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        \n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_model_1(learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "    conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    \n",
    "    flatten = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "    fc1 = fc_layer(flatten, 7 * 7 * 64, 1024, \"fc1\")\n",
    "    logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        cost = tf.reduce_mean(loss, name=\"cost\")\n",
    "        \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\"MNIST_data/demo2\") #Writes the tf.graph into DIR\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for i in range(2000):\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 500 == 0:\n",
    "            [train_accuracy] = sess.run([accuracy], feed_dict={x: batch[0], y: batch[1]})\n",
    "            print(\"Step {0}, TrainAccuracy = {1}\".format(i, train_accuracy))\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the graph on Tensorboard\n",
    "\n",
    "`tf.summary.FileWriter` is the python class that writes data for tensorboard\n",
    "\n",
    "`writer = tf.summary.FileWriter(DIR)\n",
    "writer.add_graph(sess.graph)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    mnist_model_1(learning_rate = 1e-3)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tensorboard Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**summary()** is a tensorflow op that output protocol buffers containing *summarized data*.\n",
    "There are 4 types of summaries available:\n",
    "1. `tf.summary.scalar` : Outputs Scalar Summaries\n",
    "2. `tf.summary.image` : Outputs Image Summaries\n",
    "3. `tf.summary.audio` : Outputs Audio Summaries\n",
    "4. `tf.summary.histogram` : Outputs Histogram Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model with Summaries\n",
    "\n",
    "### As the first step, Scalar Summaries are considered.\n",
    "\n",
    "Let's modify our mnist_model to display scalar summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for an example, We want to add scalar summary to cost. This is how to do it,\n",
    "`tf.summary.scalar(\"cost\", cost)`\n",
    "\n",
    "After adding summaries, They should merge. To merge all summaries collected in the default graph, Following line of code is needed to be added.\n",
    "`summ = tf.summary.merge_all()`\n",
    "\n",
    "Next,\n",
    "`writer.add_summary()` is used to add summaries to Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_model_2(learning_rate):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "    conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    \n",
    "    flatten = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "    fc1 = fc_layer(flatten, 7 * 7 * 64, 1024, \"fc1\")\n",
    "    logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        cost = tf.reduce_mean(loss, name=\"cost\")\n",
    "        ## NEWLY ADDED\n",
    "        tf.summary.scalar(\"cost\", cost) # scalar Summary for cost\n",
    "        \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        ## NEWLY ADDED\n",
    "        tf.summary.scalar(\"accuracy\", accuracy) # Scalar Summary for accuracy\n",
    "        \n",
    "    ## NEWLY ADDED\n",
    "    summ = tf.summary.merge_all() # Merging all summaries collected in the default graph\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(\"MNIST_data/demo2\") #Writes the tf.graph into DIR\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for i in range(2000):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "        if i % 500 == 0:\n",
    "            [train_accuracy] = sess.run([accuracy], feed_dict={x: batch[0], y: batch[1]})\n",
    "            print(\"Step {0}, TrainAccuracy = {1}\".format(i, train_accuracy))\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, TrainAccuracy = 0.07000000029802322\n",
      "Step 500, TrainAccuracy = 0.949999988079071\n",
      "Step 1000, TrainAccuracy = 0.9399999976158142\n",
      "Step 1500, TrainAccuracy = 0.9800000190734863\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    mnist_model_2(learning_rate = 1e-2)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's work with histograms\n",
    "To do that, Conv and FC layer defining functions are need to be modified.\n",
    "So, Modified functions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer(input, size_in, size_out, name=\"conv\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        conv = tf.nn.conv2d(input, w, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        tf.summary.histogram(\"Weights\", w)\n",
    "        tf.summary.histogram(\"Biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return tf.nn.max_pool(act, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "def fc_layer(input, size_in, size_out, name=\"fc\"):\n",
    "    with tf.name_scope(name):\n",
    "        w = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"W\")\n",
    "        b = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"B\")\n",
    "        act = tf.matmul(input, w) + b\n",
    "        tf.summary.histogram(\"Weights\", w)\n",
    "        tf.summary.histogram(\"Biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_model_3(learning_rate, DIR):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x\")\n",
    "    x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10], name=\"labels\")\n",
    "    \n",
    "    conv1 = conv_layer(x_image, 1, 32, \"conv1\")\n",
    "    conv2 = conv_layer(conv1, 32, 64, \"conv2\")\n",
    "    \n",
    "    flatten = tf.reshape(conv2, [-1, 7 * 7 * 64])\n",
    "    fc1 = fc_layer(flatten, 7 * 7 * 64, 1024, \"fc1\")\n",
    "    logits = fc_layer(fc1, 1024, 10, \"fc2\")\n",
    "    \n",
    "    with tf.name_scope(\"cost\"):\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "        cost = tf.reduce_mean(loss, name=\"cost\")\n",
    "        ## NEWLY ADDED\n",
    "        tf.summary.scalar(\"cost\", cost) # scalar Summary for cost\n",
    "        \n",
    "    with tf.name_scope(\"train\"):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "        \n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        ## NEWLY ADDED\n",
    "        tf.summary.scalar(\"accuracy\", accuracy) # Scalar Summary for accuracy\n",
    "        \n",
    "    ## NEWLY ADDED\n",
    "    summ = tf.summary.merge_all() # Merging all summaries collected in the default graph\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(DIR) #Writes the tf.graph into DIR\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    for i in range(2001):\n",
    "        batch = mnist.train.next_batch(100)\n",
    "        if i % 5 == 0:\n",
    "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: batch[0], y: batch[1]})\n",
    "            writer.add_summary(s, i)\n",
    "        if i % 500 == 0:\n",
    "            [train_accuracy] = sess.run([accuracy], feed_dict={x: batch[0], y: batch[1]})\n",
    "            print(\"Step {0}, TrainAccuracy = {1}\".format(i, train_accuracy))\n",
    "        \n",
    "        sess.run(train_step, feed_dict={x: batch[0], y: batch[1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a little trick here. We will iterate 2 learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning rate =  0.001\n",
      "Step 0, TrainAccuracy = 0.07999999821186066\n",
      "Step 500, TrainAccuracy = 0.9800000190734863\n",
      "Step 1000, TrainAccuracy = 0.9800000190734863\n",
      "Step 1500, TrainAccuracy = 0.9800000190734863\n",
      "Step 2000, TrainAccuracy = 1.0\n",
      "Training with learning rate =  0.0001\n",
      "Step 0, TrainAccuracy = 0.05999999865889549\n",
      "Step 500, TrainAccuracy = 0.9700000286102295\n",
      "Step 1000, TrainAccuracy = 0.9399999976158142\n",
      "Step 1500, TrainAccuracy = 0.9900000095367432\n",
      "Step 2000, TrainAccuracy = 0.9599999785423279\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    for lr in [1E-3, 1E-4]:\n",
    "        print(\"Training with learning rate = \",lr)\n",
    "        directory = \"MNIST_data/demo3/\" + str(lr)\n",
    "        mnist_model_3(learning_rate = lr, DIR = directory)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
